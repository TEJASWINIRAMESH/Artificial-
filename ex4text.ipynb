{"cells":[{"cell_type":"code","execution_count":null,"id":"6e9713b7-90d9-4958-8f09-c4f4a4d472e6","metadata":{"id":"6e9713b7-90d9-4958-8f09-c4f4a4d472e6"},"outputs":[],"source":["import keras\n","from keras import ops # operations for tensor manipulation functions\n","from keras import layers\n","import tensorflow as tf\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras import layers"]},{"cell_type":"code","execution_count":null,"id":"25e26d06-8a64-43b8-bdbc-589e96e71831","metadata":{"id":"25e26d06-8a64-43b8-bdbc-589e96e71831"},"outputs":[],"source":["class TransformerBlock(layers.Layer):\n","    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n","        super().__init__()\n","        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n","        self.ffn = Sequential(\n","            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n","        )\n","        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n","        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n","        self.dropout1 = layers.Dropout(rate)\n","        self.dropout2 = layers.Dropout(rate)\n","\n","    def call(self, inputs):\n","        attn_output = self.att(inputs, inputs)\n","        attn_output = self.dropout1(attn_output)\n","        out1 = self.layernorm1(inputs + attn_output)\n","        ffn_output = self.ffn(out1)\n","        ffn_output = self.dropout2(ffn_output)\n","        return self.layernorm2(out1 + ffn_output)"]},{"cell_type":"code","execution_count":null,"id":"fe112d3f-f1db-4f4c-9cb9-cd469b58e3b0","metadata":{"id":"fe112d3f-f1db-4f4c-9cb9-cd469b58e3b0"},"outputs":[],"source":["class TokenAndPositionEmbedding(layers.Layer):\n","    def __init__(self, maxlen, vocab_size, embed_dim):\n","        super().__init__()\n","        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n","        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n","\n","    def call(self, x):\n","        maxlen = ops.shape(x)[-1]\n","        positions = ops.arange(start=0, stop=maxlen, step=1)\n","        positions = self.pos_emb(positions)\n","        x = self.token_emb(x)\n","        return x + positions"]},{"cell_type":"code","execution_count":null,"id":"983f93f8-d576-4ff4-8614-d56d92212d82","metadata":{"id":"983f93f8-d576-4ff4-8614-d56d92212d82","outputId":"ab8a67b6-e470-4174-fa17-b0e6019670bd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training sequences: 25000\n","Validation sequences: 25000\n"]}],"source":["vocab_size = 20000 # only top 20K words\n","max_len = 200 # only top 200 words in each movie review\n","\n","(X_train, y_train), (X_val, y_val) = imdb.load_data(num_words=vocab_size)"]},{"cell_type":"code","execution_count":null,"id":"bf227914-9521-4866-9c40-578a43535c07","metadata":{"id":"bf227914-9521-4866-9c40-578a43535c07"},"outputs":[],"source":["X_train = pad_sequences(X_train, maxlen=max_len)\n","X_val = pad_sequences(X_val, maxlen=max_len)"]},{"cell_type":"code","execution_count":null,"id":"bc1634e4-a1df-4150-8a53-ef59a84a7019","metadata":{"id":"bc1634e4-a1df-4150-8a53-ef59a84a7019"},"outputs":[],"source":["inputs = layers.Input(shape=(max_len,))"]},{"cell_type":"code","execution_count":null,"id":"9a0e6455-1c1a-4305-9652-31d2e36af96b","metadata":{"id":"9a0e6455-1c1a-4305-9652-31d2e36af96b"},"outputs":[],"source":["embed_dim = 32\n","embedding_layer = TokenAndPositionEmbedding(max_len, vocab_size, embed_dim)\n","x = embedding_layer(inputs)"]},{"cell_type":"code","execution_count":null,"id":"c1eb110f-977a-419f-b5c2-7b6e9a0b3b74","metadata":{"id":"c1eb110f-977a-419f-b5c2-7b6e9a0b3b74"},"outputs":[],"source":["num_heads = 2 # attention heads\n","ff_dim = 32 # hidden layer size in FFN inside transformer\n","transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n","x = transformer_block(x)"]},{"cell_type":"code","execution_count":null,"id":"3967b0e8-8db7-42f5-b2e8-6b71eb9b032e","metadata":{"id":"3967b0e8-8db7-42f5-b2e8-6b71eb9b032e"},"outputs":[],"source":["x = layers.GlobalAveragePooling1D()(x)"]},{"cell_type":"code","execution_count":null,"id":"5426d0cc-1d3d-456f-a0c9-e81035799fcd","metadata":{"id":"5426d0cc-1d3d-456f-a0c9-e81035799fcd"},"outputs":[],"source":["x = layers.Dropout(0.1)(x)"]},{"cell_type":"code","execution_count":null,"id":"260b0c1b-dcb7-48ec-8d0d-a3c841c064ae","metadata":{"id":"260b0c1b-dcb7-48ec-8d0d-a3c841c064ae"},"outputs":[],"source":["x = layers.Dense(20, activation='relu')(x)"]},{"cell_type":"code","execution_count":null,"id":"2c029036-4095-47a5-97f2-453caa14d157","metadata":{"id":"2c029036-4095-47a5-97f2-453caa14d157"},"outputs":[],"source":["x = layers.Dropout(0.1)(x)"]},{"cell_type":"code","execution_count":null,"id":"080db1de-3470-4012-b645-37509ba8120a","metadata":{"id":"080db1de-3470-4012-b645-37509ba8120a"},"outputs":[],"source":["outputs = layers.Dense(2, activation='softmax')(x)"]},{"cell_type":"code","execution_count":null,"id":"c96f7bac-ac00-415f-8422-4c335bacb510","metadata":{"id":"c96f7bac-ac00-415f-8422-4c335bacb510"},"outputs":[],"source":["model = keras.Model(inputs=inputs, outputs=outputs)"]},{"cell_type":"code","execution_count":null,"id":"6ac7804a-1910-4691-8f46-903a8dc234dc","metadata":{"id":"6ac7804a-1910-4691-8f46-903a8dc234dc"},"outputs":[],"source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"id":"82c0f23b-df27-4d60-84af-7dacdbd40294","metadata":{"id":"82c0f23b-df27-4d60-84af-7dacdbd40294","outputId":"9b375841-dec4-40c9-a67a-d81a8d82c20c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 39ms/step - accuracy: 0.9618 - loss: 0.1125 - val_accuracy: 0.8633 - val_loss: 0.3425\n","Epoch 2/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 41ms/step - accuracy: 0.9798 - loss: 0.0695 - val_accuracy: 0.8514 - val_loss: 0.4933\n","Epoch 3/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 40ms/step - accuracy: 0.9875 - loss: 0.0443 - val_accuracy: 0.8328 - val_loss: 0.6589\n","Epoch 4/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 40ms/step - accuracy: 0.9891 - loss: 0.0383 - val_accuracy: 0.8416 - val_loss: 0.7101\n","Epoch 5/5\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 38ms/step - accuracy: 0.9941 - loss: 0.0230 - val_accuracy: 0.8337 - val_loss: 0.8762\n"]}],"source":["history = model.fit(X_train, y_train, batch_size=32, epochs=5, validation_data=(X_val, y_val))"]},{"cell_type":"code","execution_count":null,"id":"bf586dfc-cf6c-48ed-bd14-1023b7f546d1","metadata":{"id":"bf586dfc-cf6c-48ed-bd14-1023b7f546d1","outputId":"770698c5-289d-4a7b-ecfe-1997596e36c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n","The sentiment of the input text is: negative\n"]}],"source":["tokenizer = Tokenizer(num_words=vocab_size)\n","tokenizer.fit_on_texts(imdb.get_word_index().keys())\n","\n","\n","def predict_sentiment(text):\n","\n","    sequence = tokenizer.texts_to_sequences([text])\n","\n","    padded_sequence = pad_sequences(sequence, maxlen=200)\n","\n","    prediction = model.predict(padded_sequence)\n","    sentiment = np.argmax(prediction, axis=1)[0]\n","    sentiment_label = \"positive\" if sentiment == 0 else \"negative\"\n","    return sentiment_label\n","\n","\n","user_input = \"This movie was bad.Its is boring\"\n","sentiment = predict_sentiment(user_input)\n","print(f\"The sentiment of the input text is: {sentiment}\")\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}